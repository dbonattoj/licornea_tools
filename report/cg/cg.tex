\documentclass{scrreprt}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{listings}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\transpose}[1]{#1^\intercal}
\newcommand{\degr}{\text{\textdegree}}

\lstset{basicstyle=\footnotesize\ttfamily}
\lstset{xleftmargin=5mm}
\setcounter{secnumdepth}{5} 

\title{Camera grid extrinsic self-calibration}
\author{Tim Lenertz}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\chapter{Introduction}
This report describes the method used to calibrate the extrinsic camera parameters of the 3DLicorneA data sets. If is applicable for dense 2D data sets, where cameras are placed on a more or less regular grid on a plane parallel to the scene. The optical flow of tracked features, and aggregated values from the depth maps, are used to compute the camera positions and orientations. No calibration pattern needs to be present in the scene.

\section{Requirements}
To use the method, there are the following requirements:
\begin{itemize}
\item The intrinsic camera parameters are known. This is the camera matrix $\matr{K}$ and optionally the distortion coefficients. The method was used for the case where there is no distortion (Kinect v2), but with additional steps it can be used with distorted images.
\item The intrinsic camera parameters are the same for each image.
\item There is a depth map for all or some views. There is an extension to use the method without depth maps, and instead giving a depth only for some feature points.
\item The camera centers are arranged on an approximately regular 2D grid on a plane $P$. The distance between adjacent camera positions (in X and Y direction) is sufficiently small that feature points can be tracked using optical flow. For this disparities of features on adjacent views should be smaller than 15 pixels.
\item It is assumed that camera centers lie always exactly on the plane. That is, they never more towards or back from the scene. 
\item The camera is facing approximately perpendicular to the plane $P$, towards the scene. There can be a small rotation $\matr{R}$ of the camera relative to $P$. It is estimated from the images, as part of the calibration. The yaw, pitch and roll angles should be smaller than 5\textdegree. It is assumed that this angle remains constant for all views.
\item There can be some missing images and depth maps in the data set.
\item It is not necessary that the tracked feature points remain visible across the whole range of views. Calibration can be done on subsets of the camera positions, and then stitched together.
\end{itemize}


\chapter{Method}
The calibration is done in four steps: (1) Compute \emph{image correspondences} using feature tracking. (2) Estimate the \emph{camera rotation} $\matr{R}$. (3) Estimate \emph{straight depths} of the tracked features, i.e. their distance to $P$. (4) Deduce camera positions.

The method calculates one global rotation matrix $\matr{R}$, and for each view $v$, a 2D vector $\vec{c_v}$ of the camera center position on the plane $P$. From this it then computes the extrinsic matrix $\matr{Rt}$ for each view.

\section{Overview}
First the algorithm selects several \emph{feature points} on one on multiple \emph{reference views}. Using optical flow, it then tracks the position of the same features on the other views.

With the pin-hole camera model, it is be possible to calculate the camera position on $P$ directly from a feature point's positions on different views, if the camera is pointing perpendicular to $P$, and the feature's distance to $P$ is known. So the algorithm first needs to estimate $\matr{R}$, and the straight depths of the features.

To calculate $\matr{R}$, two methods are used. One uses a non-linear model which estimates $\matr{R}$ only from the slope of the lines that the tracked features make when the camera moves horizontally and vertically, without knowledge of the features' depths. The other method uses the depths of the features on the different views. Both estimate a full 3D rotation matrix.

The distance of a feature to $P$ is called its \emph{straight depth}. Knowing $\matr{R}$, it can now be calculated from the feature points' depths in each view's depth map. If depth maps are not available, it is also possible to fix only the depth of one or more features, and deduce the rest from the relative scales of the different feature's disparities. 

Using $\matr{R}$ and the \emph{straight depth} of each feature, the algorithm now estimates the set of camera positions on $P$, once for each feature. The results are aggregated to find the final camera positions.

The resulting camera positions are in a coordinate system with the camera of the \emph{reference view} at origin. If multiple reference views were used in the feature tracking step, the entire procedure is repeated for each reference view, and in the end the camera positions are stitched together.



\section{Preliminaries}
The 2D dataset consists of several \emph{views}. A view $v$ consists of an image, and optionally a depth map, taken from one camera position. The views are enumerated with two integer indices $v(x,y)$. Views with the same $x$ index are (approximately) aligned vertically, views with the same $y$ index horizontally. This is relative to the camera image planes.

The goal is to estimate the position and orientation of the camera for each view, i.e. to find the extrinsic camera matrices $\matr{Rt}_v$.

If depth maps are used, they need to be in the same coordinate systems as the images. For each image pixel $(i_x,i_y)$, the value $d$ of the same pixel in the depth map needs to indicate the orthogonal distance from the camera center to that object point, orthogonal to the camera image plane. The camera matrices $\matr{Rt}_v$ will be expressed in the same unit as these depths.

 
\section{Image correspondences}
First some features $f$ are selected. They correspond to 3D points in the scene. This step aims to find for each feature $f$, the set of \emph{feature points} $p(f,v) = (x,y,d,w)$, that is the pixel coordinates $x,y$ where the feature is visible in each view $v$. This data is called the \emph{image correspondences}.

A feature point optionally also contains a depth $d$, and a weight $w$. The depth is a distance orthogonal to the camera image plane of $v$. If $\matr{R} \neq \matr{I}$, then the depths $p_{f,*}$ of the same feature for different views will be different.


\subsection{Choosing feature points}
Features are chosen by choosing feature points on a \emph{reference view}. By default the center view in the dataset is used as reference view, but there can also be multiple reference views (see later).

The chosen feature points need to be such that they are likely to remain \emph{stable} when doing feature tracking. It means that when one looks for a similar-looking nearby point on an adjacent view, it is likely to be the same scene point. An example is shown on figure \ref{fig:choosefeatures}.

\begin{figure}
\includegraphics[width=\textwidth]{choosefeatures.png}
\caption{Chosen feature points}
\label{fig:choosefeatures}
\end{figure}

The \emph{OpenCV} function \texttt{goodFeaturesToTrack} is used. Additionally, the image is first subdivided into 4 or more rectangular regions, and the best chosen features from each region are taken.

The chosen features should be well distributed across the image, and have different depths. There should be about $300$ or more features, considering that many will be filtered out because their optical flow is unstable.


\subsection{Optical flow tracking}
Optical flow feature tracking is always done on adjacent views, for example $v(x,y)$ and $v(x+1,y)$. Then sequentially, it uses the corresponding feature points on $v(x+1,y)$ to estimate those for $v(x+2,y)$, and so on. So there is an error accumulation, which gets worse the longer the path that the view indices take is.

The acquisition system moves line-by-line. So it is physically guaranteed that for any $v(x,y)$ and $v(x+1,y)$, the camera only moves by small amount, whereas for $v(x,y)$ and $v(x,y+1)$, there can be a larger deviation. So it is better to take most optical flow correspondences in $x$ direction.


\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{opticalflow.pdf}
\caption{Optical flow paths}
\label{fig:opticalflow}
\end{figure}

The optical flow algorithm moves over the $(x,y)$ view indices as shown on figure \ref{fig:opticalflow}. The center view (black circle) is the reference view. As each step moving from $(x,y)$ to $(x',y')$, for each feature $f$, all the feature points $p(f,v(x',y'))$ are computed from those of $p(f,v(x,y))$. If no feature point $p(f,v(x',y'))$ could be computed anymore, the algorithm stops for that line.

If the image for a view is missing, that view is skipped, and instead the correspondences are taken from the second-previous view, as shown. It is important that no view is missing in the column of the reference view, because then that entire line will be skipped.

The reference view is not one of the edges, but instead in the center, and the optical flow steps are done in all four directions. This minimizes the total path taken, and reduces the accumulated error.

The maximal number of steps to be taken in $x$ and in $y$ direction, called \emph{outreach}, can be set to a maximal limit. Using a smaller \emph{outreach}, and instead doing the calibration from multiple reference views, and combining the results in the end, can produce better final results.

To compute the optical flow, the \emph{OpenCV} function \texttt{calcOpticalFlowPyrLK} is used. The parameters can be adjusted. It can also be set to use multi-scale image pyramids, so that larger features can be used.

\begin{figure}
\includegraphics[width=\textwidth]{ofgood1.png}
\includegraphics[width=\textwidth]{ofgood2.png}
\caption{Good feature correspondences}
\label{fig:ofgood}

\vspace{1.5cm}

\includegraphics[width=\textwidth]{ofbad2.png}
\includegraphics[width=\textwidth]{ofbad1.png}
\caption{Bad feature correspondences}
\label{fig:ofbad}
\end{figure}

Figures \ref{fig:ofgood} are examples of good feature correspondences. The background image is a close-up of a view image. The red dot is the feature point on this view. The other dots are the corresponding feature points on the other views. 

\subsection{Filtering features}
The next step is to filter the generated \emph{image correspondences}. It is important, because incorrect feature points can have a large impact on the final results. The optical flow procedure tends to generate a large number of bad correspondences. There is an algorithm to automatically filter out bad correspondences, but they should also be verified by hand.

Figures \ref{fig:ofbad} are examples of good feature correspondences. In the first example, the deviation occurred because a foreground object with a curved border moved in front of the tracked feature. Such deviations can also occur because of specular reflections (for example on the metal sink), and because of badly chosen feature (such as on the furry objects). In the second example, the pattern appears regular but the correspondence is still incorrect.

The filtering algorithm removes all feature points for one a feature $f$ entirely, if there are too little feature points, or if the pattern deviates too much from a regular lattice.

Properly filtering the correspondences is important: Having incorrect correspondences, and having too little correspondences, both have a large impact on the final result. In practice, for each view, there should remain feature points for about $100$ features.



\subsection{Reference views}
\label{sec:refviews}
If the range of motion of the camera is large, or the field of view is small, many feature points that are visible in the center view, will not be visible on the extremity views. 
When multiple reference views are used, the entire calibration will be done for each reference view independently (except for the rotation estimation, as $\matr{R}$ is constant).

Figure \ref{fig:refview} shows how reference views can be selected. The four colored dots are the reference views. They must be selected so that they are in a grid, for the stitching algorithm to work later. It is called the \emph{reference grid} (or \emph{reference views grid}). The \emph{x key} and \emph{y key} is the distance between reference view indices, in $x$ and $y$ direction.

The \emph{outreach} indicates the maximal range of view indices around the \emph{reference view}, for which correspondences will be searched. In the figure these regions are shown as the colored rectangles. For the stitching to work, there must be some overlap in these rectangles, as shown by the yellow regions in the figure.

The overlap should be large enough so that there will remain many views with feature points from two references, after the filtering. The \emph{outreach} should be kept relatively small, for two reasons: (1) The maximal propagated error from the optical flow feature tracking is larger if the outreach is large. This also makes it more likely for entire features to be filtered out, even though a smaller portion of its feature points are good. (2) When camera positions are computed at the end, the impact of the error in the camera rotation and in the feature depths, becomes much larger for views that are further away from the reference view. (see later, in section \ref{sec:campos})

For example an \emph{outreach} of $80$, and a \emph{key} of $100$ can be a good choice. It would leave an overlap range of $60$.

Given a horizontal/vertical \emph{outreach} and \emph{key}, the algorithm chooses the \emph{reference grid} so that there are no missing images on the columns of the reference views' $x$ indices.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{refview.pdf}
\caption{Reference views arrangement}
\label{fig:refview}
\end{figure}


\subsection{Feature point depths}
If depth maps are available, they are used to attribute a depth value $d$ to each feature point $p(f,v) = (x,y,d,w)$. The depth is read from the view's depth map, at pixel position $(x,y)$.

However, features are often located on the border of foreground objects. Taking a single pixel value in the depth map could incorrectly take the depth of the background, or an incorrect intermediary value. Therefore the algorithm takes a small pixel window around $(x,y)$, and retains the minimal value in it.



\subsection{Feature point weights}
Feature points $p(f,v)$ can have a weight value. If many feature points are clustered together on a small region of the image (for example a checkerboard), it is reasonable to given them a lower weight, and to give a higher weight to more isolated feature points. The weight could also depend on a confidence value calculated for the correspondence.

Especially for the rotation estimation from optical flow slopes (see later), it is important that all regions in the image are uniformly represented.

This is not implemented.

\pagebreak
\section{Observations}
Looking at the image correspondences on figures \ref{fig:ofgood}, it can be seen that the arrangement of the feature points roughly corresponds to the (inverted) camera positions on $P$. (The vertical gap on the second figure is a result of the acquisition system: It did not take the $y$-step properly at that height.) 

The feature points for every feature $f$ will be arranged in the same pattern, just at different places in the image, and with different scales (i.e. disparities). The basic idea of this calibration method is to overlay the feature points for several features, make their scale uniform, and take the averages, to get a good estimate of the camera positions.


\subsection{Rotation}
However this would only be correct if $\matr{R} = \matr{I}$. In reality, the rotation $\matr{R}$ distorts the feature points. For figure \ref{fig:onedimslopes}, a 1D optical flow was taken, with the camera moving on a horizontal axis only. The feature points were then overlaid, centered on one feature point, and given uniform scale. The figure shows these transformed feature points, one color for each feature. It can be seen that they form lines with different slopes. The scaling does not affect the slope. The different slopes are caused by the camera's rotation $\matr{R}$. In the 2D case, it is possible to calculate $\matr{R}$, from these slopes alone. This is done in section \ref{sec:camrotation}.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{onedimslopes.pdf}
\caption{Overlayed feature points in 1D case}
\label{fig:onedimslopes}
\end{figure}

\pagebreak
\subsection{Depth}
Another problem is that if more than two sets of feature points should be given a common scale, one set of feature points would need to be chosen as reference. This would amplify the error in the correspondences of that particular feature. So it is better to calculate scaling factors in a more global way. This is done with the \emph{straight depths}, in section \ref{ref:straightdepths}.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{differentdepths.pdf}
\caption{Differing feature point depths}
\label{fig:differentdepths}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{xdepths.pdf}
\caption{Feature point depths as $x$ index of view varies}
\label{fig:xdepths}
\end{figure}

Figure \ref{fig:differentdepths} shows how because of the rotation $\matr{R}$, the feature points depths $d_1, d_2$ for one feature $f$ are not all equal. Figure \ref{fig:xdepths} shows the feature point depths from one data set, of $7$ different features, as the $x$ index of the view varies. Despite the noise, a clear linear increase can be seen, with the same slope for each feature.

In section \ref{ref:straightdepths} all of these feature points depths are aggregated together, to calculate the straight depth $sd_f$ of each feature, using $\matr{R}$ and the camera intrinsic matrix $\matr{K}$.


\pagebreak
\section{Camera rotation}
\label{sec:camrotation}
The next step is to determine the camera rotation $\matr{R}$. This is the rotation of the cameras relative to the plane $P$ on which the camera centers are placed. It is a 3D rotation matrix, with 3 degrees of freedom.

If $\matr{R}$ is known to be very small, this step can be skipped entirely, by simply setting $\matr{R} = \matr{I}$. Otherwise, there are two methods to estimate $\matr{R}$.

One is based only on the \emph{slopes} of the image correspondences, and does not need depth maps. It needs the cameras to be aligned on a regular grid. It seems to produce an accuracy of around $0.5\degr$ for the three rotation angles. It is described in the next section \ref{sec:sloperot}.

The other uses the differing depths of the feature points to estimate the rotation, by doing a least squares plane fitting operation, followed by an adjustment of the roll rotation. It is described in section \ref{sec:depthrot}. It seems to give better results. Its implementation is only experimental, and there are probably some errors in it.


\subsection{Optical flow slopes}
\label{sec:sloperot}

\subsubsection{Flow equation}
The camera intrinsic matrix $\matr{K}$ projects points in the camera view space $(v_x, v_y, v_z)$, to pixel positions on the image $(i_x, i_y)$ in homogeneous coordinates, according to

\begin{equation}
w \begin{bmatrix}
	i_x \\ i_y \\ 1
\end{bmatrix}
= \begin{bmatrix}
	f_x & 0 & c_x \\
	0 & f_y & c_y \\
	0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
	v_x \\ v_y \\ v_z
\end{bmatrix}
\end{equation}

\begin{equation}
\label{eq:ofs_v_to_i}
i_x = f_x \frac{v_x}{v_z} + c_x
\hspace{1cm}
i_y = f_y \frac{v_y}{v_z} + c_y
\end{equation}

\begin{equation}
\label{eq:ofs_i_to_v}
v_x = \frac{v_z}{f_x} (i_x - c_x)
\hspace{1cm}
v_y = \frac{v_z}{f_y} (i_y - c_y)
\end{equation}

As shown on figure \ref{fig:differentdepths}, the camera moves such that the camera center is on a plane $P$, and the camera has a constant rotation $\matr{R}$ relative to $P$.

The \emph{world space} coordinate system is set such that its $z = 0$ plane is $P$, and its origin is any point $\vec{O}(0,0,0) \in P$.

Let $\vec{i}(\vec{O})$ be some point in the image of the camera when it is placed at $O$. Let $\vec{v}(\vec{O})$ be the same point in the camera's view space, calculated with formula \ref{eq:ofs_i_to_v}, with a given value $z = v_z(\vec{O})$. Let $\vec{w}$ be the same point in \emph{world space}. For any camera center position $\vec{Q} \in P$, the relation is
\begin{equation}
\vec{v}(\vec{Q}) = \matr{R} (\vec{w} + \vec{Q})
\end{equation}
To get from world space to view space, the coordinate system is first translated by $\vec{Q}$ (where $Q_z = 0$), then rotated by $\matr{R}$. In particular,
\begin{equation}
\vec{v}(\vec{O}) = \matr{R} \vec{w}
\end{equation}
hence
\begin{equation}
\vec{v}(\vec{Q}) = \vec{v}(\vec{O}) + \matr{R} \vec{Q}
\end{equation}
Using formula \ref{eq:ofs_v_to_i}, $\vec{i}(\vec{Q})$ can now be calculated from this. So one has the function
\begin{equation}
\text{flow}_{\matr{R}} : \langle \vec{i}(\vec{O}), \vec{Q}, z \rangle \mapsto \vec{i}(\vec{Q})
\end{equation} 

\subsubsection{Horizontal and vertical camera movement}
The following section will analyze how $\vec{i}(\vec{Q})$ evolves when $\vec{Q}$ moves on $P$. Most importantly, it is shown that when $\vec{Q}$ moves horizontally or vertically on $P$, then the \textbf{slope} at which $\vec{i}(\vec{Q})$ moves on the image does not depend on $z$.
\\ \\
Let $\vec{H} = (\epsilon,0,0)$ and $\vec{V} = (0,\epsilon,0)$. They represent a horizontal and vertical displacement of the camera on $P$, by some magnitude $\epsilon$.

Because of the transformation between cartesian and homogeneous coordinates in formulae \ref{eq:ofs_v_to_i} and \ref{eq:ofs_i_to_v}, the function $\text{flow}_{\matr{R}}$ cannot be expressed directly as a matrix equation. $\matr{R}$ is decomposed:
\begin{equation}
\matr{R} = \begin{bmatrix}
	r_{11} & r_{12} & r_{13} \\
	r_{21} & r_{22} & r_{23} \\
	r_{31} & r_{32} & r_{33}
\end{bmatrix}
\end{equation}

To simplify the expressions, the coordinates of the chosen image point $\vec{i}(\vec{O})$ are simply denoted $(i_x, i_y)$. Also, $z = v_z(\vec{O})$.

For the horizontal camera movement by $\vec{H}$, one gets:
\begin{equation}
i_x(\vec{H}) = \frac{i_x z + f_x r_{11} \epsilon + c_x r_{31} \epsilon}{z + r_{31} \epsilon}
\hspace{5mm} \text{and} \hspace{5mm}
i_y(\vec{H}) = \frac{i_y z + f_y r_{21} \epsilon + c_y r_{31} \epsilon}{z + r_{31} \epsilon}
\end{equation}

And for the vertical camera movement by $\vec{V}$, one gets:
\begin{equation}
i_x(\vec{V}) = \frac{i_x z + f_x r_{12} \epsilon + c_x r_{32} \epsilon}{z + r_{32} \epsilon}
\hspace{5mm} \text{and} \hspace{5mm}
i_y(\vec{V}) = \frac{i_y z + f_y r_{22} \epsilon + c_y r_{32} \epsilon}{z + r_{32} \epsilon}
\end{equation}


\subsubsection{Slopes}
It can be shown that as $\epsilon$ varies, $\vec{i}(\vec{H})$ moves on a straight line. Its slope is
\begin{equation}
s_H = \frac{i_x - i_x(\vec{H})}{i_y - i_y(\vec{H})}
\end{equation}
This expression simplifies to
\begin{equation}
s_H = \frac{i_x - i_x(\vec{H})}{i_y - i_y(\vec{H})} = \frac{f_y r_{21} + c_y r_{31} - i_y r_{31}}{f_x r_{11} + c_x r_{31} - i_x r_{31}}
\end{equation}
The variables $\epsilon$ and $z$ both vanish. $s_H$ depends only on $\matr{R}$, $\matr{K}$ and $\vec{i}(\vec{O})$.

For the vertical camera movement by $\vec{V}$, one gets the similar expression
\begin{equation}
s_V = \frac{i_y - i_y(\vec{V})}{i_x - i_x(\vec{V})} = \frac{f_x r_{12} + c_x r_{32} - i_x r_{32}}{f_y r_{22} + c_y r_{32} - i_y r_{32}}
\end{equation}
Note that $s_H$ is a slope $x/y$, whereas $s_V$ is a slope $y/x$. This is because $\matr{R}$ is expected to be near $\matr{I}$, and in that case $\vec{i}(\vec{H})$ and $\vec{i}(\vec{V})$ move almost horizontally and vertically on the images respectively, and so both slopes approach zero (and not infinity).


\subsubsection{Samples from image correspondences}
In order to apply this to estimate $\matr{R}$, the dataset must be such that the camera centers of views $v(x-1,y), v(x,y), v(x+1,y), \dots$ must be in an approximately straight line (``horizontal''). For views $v(x,y-1), v(x,y), v(x,y+1), \dots$ they must also also be an approximately straight line (``vertical''), which is perpendicular.

After the \emph{image correspondences} were computed, for each \emph{feature} $f$, a horizontal and a vertical slope $s_H(f), s_V(f)$ are estimated using line fitting on the feature point correspondences for those view indices.

This gives for each feature $f$ a sample
\begin{equation}
S_f = \langle p(f,v), s_H(f), s_V(f) \rangle
\end{equation}
$p(f,v)$ is the feature point position of $f$ for the reference view $v$. This corresponds to the $(i_x, i_y)$ from the previous formulae. If multiple reference views were used, the samples from different reference views can be put together here.


\subsubsection{Estimating camera rotation}
It is possible to estimate $\{r_{11}, r_{21}, r_{31}, r_{12}, r_{22}, r_{32}\}$ from these samples by solving two linear homogeneous least squares systems. From this $\matr{R}$ could probably be completed knowing it is an orthogonal matrix with $\det(\matr{R}) = 1$.

But this is not done in the algorithm. Instead a parametrization of $\matr{R}$ with three Euler angles $(X,Y,Z)$ is optimized with an iterative method. The error to minimize is the mean squares sum of the predicted slopes for a given $\matr{R}$, minus the measured slopes.

The parametrization $(X,Y,Z)$ of $\matr{R}$ is $\transpose{\matr{R}} = \matr{R_z}(Z) \matr{R_y}(Y) \matr{R_x}(X)$. $\transpose{\matr{R}}$ is the orientation of the camera in world space. So the \emph{roll} rotation $Z$ (around the optical axis) is performed last.

The three angles are interdependent: Say $X$ is adjusted to minimize the error. Then $Y$ is adjusted to reduce the error even more. Now $X$ is no longer at the optical setting, and needs to be readjusted.

The roll rotation has the most impact. Three golden-section searches are performed sequentially which optimize $Z$, $X$ and $Y$, in that order. The entire process is repeated iteratively until a certain error threshold. With each (outer) iteration, the tolerance and search interval of the (inner) golden-section searches are reduced.

\subsubsection{Accuracy}
On an artificially generated test dataset with a known camera rotation of $(10\degr, 20\degr, 5\degr)$, the estimated rotation was $(10.5289\degr, 20.6345\degr, 5.36933\degr)$. This artificial dataset has some random noise and outliers, $200$ features, and $30 \times 30$ views. It is hard to estimate the accuracy for real datasets because the real rotation is unknown and typically very small.


\subsection{Feature point depths}
\label{sec:depthrot}
As shown on figure \ref{fig:differentdepths}, feature points of the same feature get different depths on different views because of the rotation $\matr{R}$. The figure shows the situation in world space. There is one global position of the feature $f$, and different positions of the camera centers $v_1, v_2$.

Figure \ref{fig:differentdepthsview} shows the same situation, but instead in the overlaid view spaces of the two cameras. The plane formed by the feature points for $f$ in the different view spaces, has the same orientation $\matr{R}$.

This algorithm calculates for each feature $f$, the view space coordinates of the feature points $p(f,v_i) = \langle x,y,d,w\rangle$, using $\vec{v}_{f,v_i} = \matr{K}^{-1} (d x, d y, d)$. Then, using linear least squares, a plane it fitted to the points $\vec{v}_{f,v_i}$, and its normal vector $\vec{n}_f$ is computed. This plane fitting should average out the noise in the depth measurements.

Then the computed normal vectors $\vec{n}_f$ for each feature $f$ are averaged to get a global estimate $\vec{n}$. A preliminary rotation matrix $\matr{R}_{xy}$ is derived from $\vec{n}$. It contains the correct pitch and yaw rotation of the camera relative to $P$, but not the correct roll.

Now again for each feature $f$, the view space points $\vec{v}_{f,v_i}$ are taken, and premultiplied by $\matr{R}_{xy}$. Using only the $x$ and $y$ components of the resulting vectors, a 2D rotation angle $\alpha_f$ is estimated such that those 2D points become horizontally/vertically aligned.

Again an average $\alpha$ is computed and transformed into a 3D roll rotation matrix $\matr{R}_z$.

Finally $\matr{R}_{xy}$ and $\matr{R}_z$ are combined into $\matr{R}$.

There might be some problems left with this implementation. On the artificially generated test dataset (same as previously), a rotation of $(9.93871\degr, 19.8223\degr, 5.22812\degr)$ was estimated. The correct values are $(10\degr, 20\degr, 5\degr)$. So the method appears to be more accurate.


\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{differentdepthsview.pdf}
\caption{View of figure \ref{fig:differentdepths} from overlaid view space coordinate systems}
\label{fig:differentdepthsview}
\end{figure}


\pagebreak

\section{Straight depths}
\label{ref:straightdepths}
Knowing $\matr{R}$, the \emph{straight depth} of each feature $f$ can now be calculated. It is the orthogonal distance of the feature point in the scene, to the plane $P$. This is the distance $\text{sd}$ in figure \ref{fig:differentdepths}. In can be samples from each feature point distance $d_i$, using the rotation $\matr{R}$ and the camera intrinsic matrix $\matr{K}$.

Two methods are available for estimating the \emph{straight depth} $sd_f$ of each feature. One aggregates the measured feature point distances $d_i$. Another estimates the straight depths from the relative scales of the feature points, whereby one or more straight depths are fixed manually.

In the case where no depth maps are available, the second method can be used. Some (at least one) depth then need to be determined manually. If depth maps are available, the second method can still be used to complete or correct estimations from the first method.

\subsection{Aggregating feature point depths}
\label{sec:sdepth_from_depth}
For each feature point $p(f,v) = \langle x,y,d,w \rangle$ which has a feature point depth $d$, the straight depth of that feature $sd_{f}$ can be estimated.

The feature point is first back-projected into the camera's view space with $\vec{v} = \matr{K}^{-1} (d x, d y, d)$. Then it is transformed into the view space of a camera with the same optical center, but perpendicular to $P$. This is $\vec{v'} = \transpose{\matr{R}} \vec{v}$. The straight depth is the third component of $\vec{v'}$.

For each feature $f$, samples $sd_{f,v}$ are calculated in this way. They should theoretically be all the same, but due to noise, outliers, and the error in $\matr{R}$, they will differ. At least the noise part of the error can be removed by averaging the samples.

To also remove the influence of outliers, the following procedure is used: First the median of the samples $sd_{f,v}$ is computed. It is not affected by outliers, but by the noise. Then the average of the $sd_{f,v}$, whose absolute difference to the median is below a given threshold $t$, is taken. This average if used as the final $sd_f$. $t$ is set to $10 \text{ mm}$. As a metric of accuracy, the standard deviation of these samples is also taken.

\subsection{Depth from disparity}
This alternate method computes straight depths using only the relative scales of the different features' disparities, and some fixed feature depth given as input. It proceeds in three steps: (1) Calculate the relative scale for each pair of features. (2) Derive a global scale for each feature. (3) Using at least one \emph{known depth}, calculate the depth of each feature.

First the feature points $p(f,v)$ of each feature are undistorted (if any distortion) and unrotated. For this the image coordinates are premultiplied by $\matr{K} \transpose{\matr{R}} \matr{K}^{-1}$. This is invariant of the feature depths, and $d$ is fixed to $1$.

\subsubsection{Pairwise scale ratios}
The feature points of each feature $f_i$ now all have the same pattern, except for a different scale, a different position in the image, and a different subset of covered views.

For each pair of features $(f_\text{i}, f_\text{j})$, a relative scale $r_{j\rightarrow i}$, and a translation $\vec{t} \in \mathbb{R}^2$ are computed which make the two \emph{feature point} sets overlap, by optimizing
\begin{equation}
\forall v : p(f_i, v) = r_{j\rightarrow i} p(f_j, v) + \vec{t}
\end{equation}

If the two features were taken on different \emph{reference views}, then this linear least squares problem is solved for $r_{j\rightarrow i}$ and $\vec{t}$.

Otherwise, if the two features were taken on the same \emph{reference view} $rv$, then is is known that $p(f_i, rv)$ and $p(f_j, rv)$ should coincide perfectly: They represent the same view, and they were initially chosen as feature points, so they have no error. So the feature point positions for $fv$ are fixed, and foreach view $v \neq rv$, a sample of $s$ is calculated using
\begin{equation*}
\foreach v : r_{v,j\rightarrow i} = \frac{p(f_i, rv) - p(f_i, v)}{p(f_j, rv) - p(f_j, v)}
\end{equation*}

Samples closer to the $rv$ are given a lower weight. They have more error because of the limited pixel resolution. The samples are then averaged to obtain $r_{j\rightarrow i}$. The translation is set to $\vec{t} = p(f_i, rv) - s \times p(f_j, rv)$.

The resulting $r_{j\rightarrow i}$ is discarded or assigned a lower weight if the error of the solution is too large. The resulting $\vec{t}$ is not further used.

Calculating $r_{j\rightarrow i}$ for each feature pair $(f_\text{i}, f_\text{j})$ gives a \emph{pairwise scales matrix}, as seen shown in figures \ref{fig:ratios} and \ref{fig:ratios_multiref}. The axis are the feature indices $i$ and $j$. Because each unordered pair is considered only once, the matrix is triangular. When there are multiple reference views, features from two different reference views have none (or little) feature points in common. This causes the black areas in the lower-left part in figure \ref{fig:ratios_multiref}.


\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{ratios.png}
\caption{Pairwise feature scales matrix}
\label{fig:ratios}

\vspace{1cm}

\includegraphics[width=0.6\textwidth]{ratios_multiref.png}
\caption{Pairwise feature scales matrix (multiple reference views)}
\label{fig:ratios_multiref}
\end{figure}


\subsubsection{Global scale ratios}
The next step is to deduce global scales $\{ r_{f_0}, r_{f_1}, r_{f_2}, ... \}$ from these samples. The \emph{global scale} of one (arbitrarily chosen) feature $f_0$ is set to $r_{f_0} = 1$.

Then all the global scales are calculated such that
\begin{equation*}
\forall i,j : \frac{r_{f_j}}{r_{f_i}} = r_{j\rightarrow i}
\end{equation*}

This is done by solving a sparse linear least-squares system. The system consists of equations of the form $r_{f_j} \times r_{j\rightarrow i} - r_{f_i} = 0$, and one $r_{f_0} = 1$. So the $\matr{A}$ matrix is sparse, and has one column for each relative scale ratio, and one row for each feature.


\subsubsection{Depths}
The scale ratios relate directly to the depths: Using the pin-hole camera model, one gets
\begin{equation*}
\forall i,j : \frac{sd_i}{sd_j} = \frac{r_{f_j}}{r_{f_i}}
\end{equation*}
where $sd_i, sd_j$ are the straight depths of the features $f_i, f_j$. As a consequence, there is one global $s$ such that
\begin{equation*}
\forall i : sd_{i} = s \times r_{f_j}
\end{equation*}
One or multiple \emph{known depths} $\{ sd'_0, sd'_1, ... \}$ need to be given as input. From these, $s$ is calculated using 
\begin{equation*}
s = \frac{\sum_{i} sd'_{i}}{\sum_{i} r_{f_i}}
\end{equation*}

\noindent Then, the remaining $sd_i$ can be computed using $s$ and the global scales $r_{f_i}$.

\subsubsection{Results}
Figure \ref{fig:depthcomp} shows the straight depth of $135$ features, sorted in ascending order. The ``measured'' (red) line are the depth values taken from the depth maps, using the previous method in section \ref{sec:sdepth_from_depth}. The ``calculated'' (blue) line are the same depth values, calculated only using the relative scales of the feature points. Only $3$ \emph{known depth} values were taken to compute these values.

The room mean square error in this example is about $17.60 \text{ mm}$. This error can be both in the measured and in the calculated depth values. This graph does not indicate whether the measured or the calculated values are wrong: The ``measured'' line is smooth only because the samples have been sorted by the ``measured'' values.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{depthcmp.pdf}
\caption{Calculated v. measured straight depths}
\label{fig:depthcomp}
\end{figure}

\pagebreak

\section{Camera positions}
\label{sec:campos}
Finally, having \emph{image correspondences}, the \emph{camera rotation}, and the \emph{straight depths}, the camera center positions can be computed.

\subsection{Relative camera positions}
For each \emph{reference view}, a set of \emph{relative camera positions} $c_v$ will be computed. It is the position of the camera center for view $v$, relative to that of the reference view. If there are multiple \emph{reference views}, the process is repeated for each reference view. The different sets of \emph{relative camera positions} will then be \emph{stitched} together in the next step.

If there is distortion in the camera intrinsics, all feature points $p(f,v)$ are first undistorted. Then they are unrotated, by premultiplication with $\matr{K} \transpose{\matr{R}} \matr{K}^{-1}$.

The camera position for the reference view $rv$  itself is always set to $c_{rv} = (0, 0)$. 

For each other \emph{target} view $v$, for each of its feature points $p(f,v)$, a sample $c_{f,v} = (x, y)$ of the camera position is calculated using
\begin{equation*}
c_{f,v} = \left( \frac{\left[p_x(f,v) - p_x(f,rv)\right] \times sd_f}{f_x}, \frac{\left[p_y(f,v) - p_y(f,rv)\right] \times sd_f}{f_y} \right)
\end{equation*}
where $f_x, f_y$ are the focal lengths from the camera intrinsic matrix $\matr{K}$. This is derived directly from the pin-hole camera model.

In theory all $c_{*,v}$ should have the same value (for the same $v$), but there can be a large deviation because of errors in the image correspondences, rotation or straight depth. The deviations necessarily get larger the further the target view $v$ is from the reference view $rv$. This is why in section \ref{sec:refviews}, the \emph{outreach} of the optical flow needed to be kept small.





\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{rcpos.png}
\caption{Computed camera positions (for each feature, and average)}
\label{fig:rcpositions}
\end{figure}



\subsection{Stitching}
If only one \emph{reference view} was used, this completes the calibration process.

If multiple \emph{reference views} were used, the previous step is repeated for each reference view, giving each time \emph{relative camera positions} with this reference view at origin. To stitch these together into \emph{absolute camera positions} in a common coordinate system, the absolute camera positions of the reference views need to be determined.

As shown in section \ref{sec:refviews}, the reference views need to be in a grid. (Allowing for arbitrarily chosen reference views would render this algorithm more complex.) The center reference view is chosen as origin, its \emph{absolute camera position} is set to $(0,0)$. The distances between horizontally and vertically adjacent reference view camera positions, is computed by comparing \emph{relative camera positions} that exist for both reference views.

For this to work the reference views grid in section \ref{sec:refviews} needed to be set so that the overlap is big enough. Note that for the optical flow, the \emph{outreach} is an upper bound, not all features are tracked that far, and many are filtered out. This algorithm only compares horizontally and vertically adjacent reference views, no others.


\subsection{Final camera extrinsics}
The final $\matr{Rt}$ extrinsic camera matrix to compute is a transformation from \emph{world space} to \emph{view space}, such that
\begin{equation*}
v = \matr{R} w + \vec{t}
\end{equation*}
The world space can be any coordinate system, but must be the same for every view.

For this method, world space is set to lie on the plane $P$. The \emph{absolute camera positions} are translations in world space, where the third component is set to $z = 0$. The computed $\matr{R}$ is the orientation of the camera in world space. The transformation is
\begin{equation*}
v = \matr{R} (w + \vec{c_v})
\end{equation*}
To obtain the final $\matr{Rt}$ matrices, $\matr{R}$ is unchanged, and the translation is set to $\vec{t} = \matr{R} \vec{c_v}$.


\chapter{Usage}
The above method is implemented as part of the \texttt{licornea\_tools} package. Tools in \texttt{calibration} that are prefixed with \texttt{cg} are specific to it.

\section{Preliminaries}
Before doing the \emph{camera grid} calibration, the following needs to be done:
\begin{itemize}
\item Prepare \texttt{parameters.json} dataset parameters file for the dataset. It indicates the index ranges (and steps), and the locations and file name formats of the images and depths.
\item Prepare \texttt{intr.json} file with intrinsic parameters of the camera. It can contain distortion coefficients, if the images (and depths) are distorted. But this was not tested.
\item Do the reprojection of the depth maps. The reprojected depth maps will be at the location indicated by \texttt{depth\_filename\_format} in the root group of the dataset parameters.
\end{itemize}



\section{Image correspondences}

\subsection{Reference grid}
Firstly, the \emph{references view grid} can be chosen using
\begin{lstlisting}[language=bash]
calibration/cg_choose_refgrid parameters.json 200 100 refgrid.json
\end{lstlisting}
This computes a \emph{reference grid} (indices of the \emph{reference views}), and puts it into \texttt{redgrid.json}. Here, the \emph{horizontal key} is $200$, and the \emph{vertical key} is $100$. If the keys are larger than the horizontal and vertical ranges, the \emph{reference grid} will consist only of one reference view. The program chooses the \emph{reference views} such that there are no missing views on each vertical axis.

\subsection{Undistort images (if applicable)}
If there is distortion in the images (defined in the intrinsic parameters file), then an image can be undistorted using
\begin{lstlisting}[language=bash]
calibration/undistort_image in_image.png out_image.png intr.json texture
\end{lstlisting}
For the depth maps, use \texttt{depth} instead of \texttt{texture}. It will then use nearest neighbor interpolation. This needs to be done for each image and each depth map in the dataset.s.

Alternately, it is also possible to compute the optical flow on the distorted images, and undistort the \emph{image correspondences} later.

\subsection{Optical flow features}
Now the feature points on the reference views to track are selected using
\begin{lstlisting}[language=bash]
calibration/cg_optical_flow_features parameters.json refgrid.json of/
\end{lstlisting}
It displays a graphical user interface, and the parameters can be adjusted such that good feature points get selected for all reference views. Hitting \emph{Enter} puts a file \texttt{of/fpoints\_*.json} into the \texttt{of/} directory, for each containing the feature points for each reference view.

All of the features will get globally unique names of the form \texttt{feat\_RFFF}, where \texttt{R} is the number of the reference view, and \texttt{FFF} the number of the feature.

\subsection{Optical flow correspondences}
Now the optical flow correspondences can be computed, using
\begin{lstlisting}[language=bash]
calibration/cg_optical_flow_cors parameters.json of/fpoints_100,100.json
250 150 cors_100,100.bin
\end{lstlisting}
This computes the \emph{image correspondences} from the optical flow for the reference view feature points \texttt{of/fpoints\_100,100.json}. They get written into \texttt{cors\_100,100.bin}. If there are multiple reference views, this process needs to be repeated for each one, by calling the program once for each file in \texttt{of/}.

In this example, the \emph{horizontal outreach} is $250$, and the \emph{vertical outreach} is $150$.

This process takes by far the most time in the calibration process. It will open each image file once. The \emph{image correspondences} output file can have a \texttt{.bin} or a \texttt{.json} filename extension. If \texttt{.bin} is used, they are stored in a binary format that takes up less disk space, and can be read faster. This is useful for large datasets.

\noindent To copy the \emph{image correspondences} from/to binary format, use
\begin{lstlisting}[language=bash]
calibration/copy_cors cors_100,100.json cors_100,100.bin
\end{lstlisting}

\noindent Information about the \emph{image correspondences} can be obtained using
\begin{lstlisting}[language=bash]
calibration/cors_info parameters.json cors_100,100.bin
\end{lstlisting}

\subsection{Merge image correspondences}
The \emph{image correspondences} computed for the different \emph{reference views} should now be merged into one file, using
\begin{lstlisting}[language=bash]
calibration/merge_cors cors_100,100_f2.bin cors_200,100_f2.bin cors_all.bin
\end{lstlisting}

\noindent If there are more than two reference views it should be called multiple times. The resulting file will still contain the information about the different reference views. This makes the following steps easier. All of the programs are aware that there can be features with different reference views in the image correspondences file.


\subsection{Visualizing image correspondences}
The resulting \emph{image correspondences} can be visualized in two ways:

To see the all the feature points on one view, use
\begin{lstlisting}[language=bash]
calibration/cg_cors_viewer_v cors_all.bin
\end{lstlisting}
It displays a graphical user interface where the view to show can be ajusted.

To see all the feature points of one feature, use
\begin{lstlisting}[language=bash]
calibration/cg_cors_viewer_f cors_all.bin 
\end{lstlisting}

\noindent It displays a graphical user interface where the feature to show can be adjusted. It displays a dot for each position of this feature (on a different view). The (backdrop) view to show can also be adjusted. With
\begin{lstlisting}[language=bash]
calibration/cg_cors_viewer_f cors_100,100.bin closeup
\end{lstlisting}

\noindent It instead displays a closeup view of the image, with only the area where the feature points are placed. It can also show the corresponding depth map as overlay. Figures \ref{fig:ofgood} and \ref{fig:ofbad} were generated with this.



\subsection{Feature point depths (if applicable)}
The feature point depths can be added to the \emph{image correspondences} using
\begin{lstlisting}[language=bash]
calibration/read_feature_depths cors_all.bin
cors_all_with_depths.bin
\end{lstlisting}

\noindent It will open each (reprojected) depth map file once, which can also take a lot of time. The image correspondences with depth are stored into \texttt{cors\_all\_with\_depth.bin} in this example. The output file can also be the same as the input file (then it will replace it).


\subsection{Filtering image correspondences}
The feature points should be filtered both automatically and manually.
First use
\begin{lstlisting}[language=bash]
calibration/cg_filter_features parameters.json cors_all.bin
cors_all_f.bin 125 75 use_depth
\end{lstlisting}
to filter out most obvious bad image correspondences. In this example $125$ and $75$ are the number of \emph{expected feature points} in horizontal and vertical directions. It should be the \emph{outreach} divided by two. (Because if the reference view is close to the border, only half of the outreach can be done). If \texttt{use\_depth} is provided, it also checks the constancy of the feature depths. It should be set, unless the calibration is done without depth maps. Some hardcoded parameters in \texttt{src/calibration/cg\_filter\_features.cc} probably need to be adjusted to get good results for a particular data set.

To filter out the remaining bad features, \texttt{cg\_cors\_viewer\_f} should be used in \texttt{closeup} mode. The names of the features to remove should be noted. Then, use
\begin{lstlisting}[language=bash]
calibration/remove_cors cors_all_f.bin cors_all_f2.bin 
feat1003,feat1010,feat2110,feat3001
\end{lstlisting}
to filter out those feature, and write the remaining image correspondences into the file \texttt{cors\_all\_2.bin}.


\subsection{Undistort image correspondences (if applicable)}
If there is distortion (defined in the intrinsic parameters file), and the images were not undistorted before, and \emph{image correspondences} can be undistorted now, using
\begin{lstlisting}[language=bash]
calibration/undistort_cors cors_all_f2.bin
cors_all_f2_undist.bin intr.json
\end{lstlisting}

\noindent It puts each feature point to the position where it would be if the image had been undistorted before the optical flow computation. The \emph{undistorted} image correspondences must be used for the subsequent step




\section{Rotation estimation}
As described before, the rotation estimation can be done using the slopes of the optical flow (when no depth maps are available), or using the feature point depths.


\subsection{Measuring optical flow slopes}
The optical flow slopes on the image correspondences can be measured using
\begin{lstlisting}[language=bash]
calibration/cg_measure_optical_flow_slopes parameters.json cors_all_f2.bin
intr.json slopes.json
\end{lstlisting}

\noindent It will measure a horizontal and a vertical slope for each feature point from each reference view, and write it into \texttt{slopes.json}.


\subsection{Visualizing optical flow slopes}
To visualize optical flow slopes (actual and model), use
\begin{lstlisting}[language=bash]
calibration/cg_slopes_viewer parameters.json intr.json slopes.json
\end{lstlisting}
where \texttt{slopes.json} are measured optical flow slopes. If there are no measured optical flow slopes, a feature points file can also be given as input, such as \texttt{of/fpoints\_100,100.json}.

It displays a graphical user interface where the model Euler angles can be adjusted, and the modelled slopes are displayed, along with the measured slopes. This can be used to manually estimate Euler angles that correspond to the measured optical flow slopes.


\subsection{Optimizing optical flow slopes}
To estimate a camera rotation $\matr{R}$ using the measured optical flow slopes, use
\begin{lstlisting}[language=bash]
calibration/cg_rotation_from_fslopes intr.json slopes.json R.json
\end{lstlisting}

\noindent It will save the estimated rotation matrix into \texttt{R.json}.


\subsection{Rotation from depths}
To instead estimate the rotation using the feature points depths, use
\begin{lstlisting}[language=bash]
calibration/cg_rotation_from_depths cors_all_f2.bin intr.json R.json
\end{lstlisting}


\section{Straight depths}
\subsection{Aggregate feature point depths}
To calculate the feature point \emph{straight depths} using the feature points depths, use
\begin{lstlisting}[language=bash]
calibration/cg_straight_depths_from_depths cors_all_f2.bin R.json
depths.json
\end{lstlisting}

\subsection{Depth from disparity}
To estimate the straight depths using only the relative scales of the feature points, use
\begin{lstlisting}[language=bash]
calibration/cg_straight_depths_from_disparity cors_all_f.bin intr.json
R.json some_depths.json depths.json
\end{lstlisting}

\noindent The file \texttt{some\_depths.json} needs to contain at least one measured \emph{straight depth}, or more to get a better fit. If no depth maps are available, it can for example be obtained manually using laser distance measurement on one of the chosen feature points of a reference view.

This can also be used to complete the straight depths obtained from aggregating feature point depths: Two image correspondences files \texttt{cors\_all\_f.bin} and \texttt{cors\_all\_fd.bin} are maintained. The latter has been filtered with the \texttt{use\_depth} option when using \texttt{calibration/cg\_filter\_features}, the formed without it. 

Then \texttt{cg\_straight\_depths\_from\_depths} is executed on \texttt{cors\_all\_fd.bin}. To also obtain straight depth for the additional image correspondences that remain in the file \texttt{cors\_all\_f.bin}, \texttt{cg\_straight\_depths\_from\_disparity} is now used, whereby the previously obtained straight depth are given as \texttt{some\_depths.json}. 



\section{Camera positions}
To compute the \emph{relative camera positions}, for each reference view, use
\begin{lstlisting}[language=bash]
calibration/cg_rcpos_from_cors parameters.json cors_all_f2.bin intr.json
R.json depths.json rcpos.json
\end{lstlisting}

\noindent The resulting \texttt{rcpos.json} file will contain \emph{relative camera positions} for each reference view. There is no need to run the program multiple times.

To stitch the relative camera positions together and obtain the final camera parameters, use
\begin{lstlisting}[language=bash]
calibration/cg_stitch_cameras refgrid.json rcpos.json intr.json R.json
cams.json
\end{lstlisting}

\noindent Here \texttt{refgrid.json} is the \emph{reference grid} file chosen as the first step. This needs to be done even if there is only one reference view. \texttt{cams.json} will contain the final camera parameters.

To visualize the camera parameters, use
\begin{lstlisting}[language=bash]
camera/visualize cams.json cams.ply world 0.3
\end{lstlisting}

\noindent It will generate the PLY file \texttt{cams.ply} containing a 3D visualization of the cameras in world space. $0.3$ is the size of the cameras in this visualization.

To convert the camera parameters into the format and convention used by VSRS, use
\begin{lstlisting}[language=bash]
camera/export_mpeg cams.json cams.txt
\end{lstlisting}




\end{document}
